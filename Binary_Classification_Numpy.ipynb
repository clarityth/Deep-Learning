{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKelMv2k5gNx",
        "outputId": "f37f3395-7d45-4f4d-9b15-f1b3d7cb8721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObQrvnbB8hPZ",
        "outputId": "92a5ed3b-4863-4ab8-d6ae-835c5f42153a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300\n"
          ]
        }
      ],
      "source": [
        "# Read file \"hw1_data.tsv\"\n",
        "file = open('/content/drive/MyDrive/hw1_data.tsv', 'r')\n",
        "ori_data = file.read().strip().split(\"\\n\")\n",
        "data = []\n",
        "for item in ori_data:\n",
        "  item = item.split(\"\\t\")\n",
        "  if len(item[0]) >0:\n",
        "    data.append((item[0],item[1]))\n",
        "print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSiVjnSKBlcK",
        "outputId": "a88a4965-a0c1-41ae-f544-523861bd1cae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "Total number of datasets\n",
            "300\n",
            "300\n",
            "**************************************************\n"
          ]
        }
      ],
      "source": [
        "### Training data pre-processing\n",
        "texts, labels = [], []\n",
        "label2idx = {\"0\":0, \"1\":1}\n",
        "for i, item in enumerate(data):\n",
        "  text = item[0]\n",
        "\n",
        "  ## Preprocessing (if you want to add, please add more)\n",
        "  ################################################################################\n",
        "  text = text.replace(\"  \",\" \") ## Replace double space\n",
        "  text = text.replace(\",\", \"\") ## Replace comma to \"\"\n",
        "  text = text.lower()  ## Lower cases\n",
        "  ################################################################################\n",
        "  label = label2idx[item[1]]\n",
        "\n",
        "  texts.append(text)\n",
        "  labels.append(label)\n",
        "\n",
        "print(\"*\"*50)\n",
        "print(\"Total number of datasets\")\n",
        "print(len(texts))\n",
        "print(len(labels))\n",
        "print(\"*\"*50)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZyzodaGovkH",
        "outputId": "64d95088-6cbd-4385-e163-1aa5edb62949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Dataset Examples\n",
            "['speak for it while it forces you to ponder anew what a movie can be ', 'enables shafer to navigate spaces both large ... and small ... with considerable aplomb ', 'takes chances that are bold by studio standards ']\n",
            "(0, 1, 1)\n",
            "**************************************************\n",
            "(0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0)\n",
            "(1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0)\n"
          ]
        }
      ],
      "source": [
        "#### Split into train/dev/test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "### Write a code for collecting samples for each class\n",
        "################################################################################\n",
        "pos,neg = [], []\n",
        "train_texts, dev_texts, test_texts, train_labels, dev_labels, test_labels = [], [], [], [], [], []\n",
        "for a,b in zip(texts,labels):\n",
        "  if b == 1:\n",
        "    pos.append((a,b))\n",
        "  else:\n",
        "    neg.append((a,b))\n",
        "\n",
        "for a,b in pos[:50]:\n",
        "  test_texts.append(a)\n",
        "  test_labels.append(b)\n",
        "for a,b in neg[:50]:\n",
        "  test_texts.append(a)\n",
        "  test_labels.append(b)\n",
        "\n",
        "for a,b in pos[50:100]:\n",
        "  dev_texts.append(a)\n",
        "  dev_labels.append(b)\n",
        "for a,b in neg[50:100]:\n",
        "  dev_texts.append(a)\n",
        "  dev_labels.append(b)\n",
        "\n",
        "for a,b in pos[100:]:\n",
        "  train_texts.append(a)\n",
        "  train_labels.append(b)\n",
        "for a,b in neg[100:]:\n",
        "  train_texts.append(a)\n",
        "  train_labels.append(b)\n",
        "################################################################################\n",
        "\n",
        "\n",
        "tmp = list(zip(train_texts,train_labels))\n",
        "import random\n",
        "random.shuffle(tmp)\n",
        "train_text, train_labels = zip(*tmp)\n",
        "\n",
        "tmp = list(zip(dev_texts,dev_labels))\n",
        "import random\n",
        "random.shuffle(tmp)\n",
        "dev_texts, dev_labels = zip(*tmp)\n",
        "\n",
        "tmp = list(zip(test_texts,test_labels))\n",
        "import random\n",
        "random.shuffle(tmp)\n",
        "test_texts, test_labels = zip(*tmp)\n",
        "\n",
        "print(\"Train Dataset Examples\")\n",
        "print(train_texts[:3])\n",
        "print(train_labels[:3])\n",
        "print(\"*\"*50)\n",
        "print(train_labels)\n",
        "print(dev_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4wmL0jRBw2_",
        "outputId": "d22e290a-4593-4081-e0b9-8c428f0db286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, '<unk>': 1, 'the': 2, 'and': 3, 'a': 4, 'to': 5, '.': 6, 'of': 7, 'it': 8, 'in': 9, 'that': 10, 'is': 11, \"'s\": 12, 'as': 13, 'for': 14, 'you': 15, 'with': 16, 'film': 17, 'one': 18, 'movie': 19, 'by': 20, 'this': 21, 'its': 22, '--': 23, '...': 24, 'have': 25, 'so': 26, 'good': 27, 'story': 28, 'most': 29, 'if': 30, \"n't\": 31, 'your': 32, 'time': 33, 'no': 34, 'more': 35, 'not': 36, 'be': 37, 'takes': 38, 'are': 39, 'way': 40, 'an': 41, 'little': 42, 'on': 43, 'about': 44, 'very': 45, '(': 46, ')': 47, 'own': 48, 'but': 49, 'i': 50, 'too': 51, 'can': 52, 'from': 53, 'some': 54, 'at': 55, '``': 56, 'picture': 57, 'or': 58, 'just': 59, 'worst': 60, 'when': 61, 'could': 62, 'his': 63, 'comedy': 64, 'what': 65, 'surprisingly': 66, 'does': 67, 'better': 68, 'old': 69, 'life': 70, 'into': 71, \"''\": 72, 'moments': 73, 'watching': 74, 'like': 75, 'recent': 76, 'all': 77, 'many': 78, 'their': 79, 'every': 80, 'hate': 81, 'work': 82, 'completely': 83, 'men': 84, 'small': 85, 'considerable': 86, 'aplomb': 87, 'sensitive': 88, 'ride': 89, 'talented': 90, 'poignant': 91, 'superb': 92, 'seems': 93, 'even': 94, 'may': 95, 'once': 96, 'epic': 97, 'derivative': 98, 'point': 99, 'worth': 100, 'part': 101, 'kind': 102, 'who': 103, 'has': 104, 'line': 105, 'charm': 106, 'late': 107, 'heart': 108, 'noir': 109, 'crime': 110, 'rock': 111, 'tribute': 112, 'uses': 113, 'sensational': 114, 'will': 115, 'laughs': 116, 'make': 117, 'well': 118, 'depth': 119, 'only': 120, 'fear': 121, 'whole': 122, 'mess': 123, 'feels': 124, 'seen': 125, 'horror': 126, 'he': 127, 'despite': 128, 'amounts': 129, 'digital': 130, 'dumb': 131, 'fun': 132, 'much': 133, 'romance': 134, 'was': 135, ':': 136, 'wildly': 137, 'step': 138, 'itself': 139, 'serves': 140, 'which': 141, 'unfortunately': 142, 'through': 143, 'awful': 144, 'look': 145, 'effects': 146, 'than': 147, 'two': 148, 'want': 149, 'new': 150, 'plot': 151, 'being': 152, 'again': 153, 'dialogue': 154, 'suspense': 155, 'would': 156, 'beautiful': 157, 'scene': 158, 'cuteness': 159, 'attractive': 160, 'watch': 161, 'sketchy': 162, 'makes': 163, 'together': 164, 'any': 165, 'silly': 166, 'outrageous': 167, 'best': 168, 'sometimes': 169, 'think': 170, 'family': 171, 'young': 172, 'minutes': 173, 'justify': 174, 'theatrical': 175, 'simulation': 176, 'death': 177, 'camp': 178, 'auschwitz': 179, 'ii-birkenau': 180, 'cinematic': 181, 'how': 182, 'special': 183, 'put': 184, 'conviction': 185, 'jason': 186, 'almost': 187, 'history': 188, 'enough': 189, 'year': 190, 'flaws': 191, 'after': 192, 'out': 193, 'pay': 194, 'speak': 195, 'while': 196, 'forces': 197, 'ponder': 198, 'anew': 199, 'enables': 200, 'shafer': 201, 'navigate': 202, 'spaces': 203, 'both': 204, 'large': 205, 'chances': 206, 'bold': 207, 'studio': 208, 'standards': 209, 'manages': 210, 'infuse': 211, 'rocky': 212, 'path': 213, 'sibling': 214, 'reconciliation': 215, 'flashes': 216, 'warmth': 217, 'gentle': 218, 'humor': 219, 'working': 220, 'script': 221, 'co-written': 222, 'gianni': 223, 'romoli': 224, 'ozpetek': 225, 'avoids': 226, 'pitfalls': 227, \"'d\": 228, 'expect': 229, 'such': 230, 'potentially': 231, 'sudsy': 232, 'set-up': 233, 'enjoy': 234, 'entirely': 235, 'persuasive': 236, 'give': 237, 'exposure': 238, 'performers': 239, 'leavened': 240, 'breathless': 241, 'anticipation': 242, 'significantly': 243, 'bewilderingly': 244, 'brilliant': 245, 'entertaining': 246, 'smooth': 247, 'professional': 248, 'technically': 249, 'positive': 250, 'change': 251, 'tone': 252, 'here': 253, 'recharged': 254, 'him': 255, 'pays': 256, 'earnest': 257, 'homage': 258, 'turntablists': 259, 'beat': 260, 'jugglers': 261, 'schoolers': 262, 'current': 263, 'innovators': 264, 'kinetic': 265, 'teeming': 266, 'cranky': 267, 'adults': 268, 'rediscover': 269, 'quivering': 270, 'kid': 271, 'inside': 272, 'laughable': 273, 'compulsively': 274, 'watchable': 275, 'merit': 276, '103-minute': 277, 'length': 278, 'value': 279, 'respect': 280, 'term': 281, 'cinema': 282, 'showing': 283, 'honest': 284, 'emotions': 285, 'comic': 286, 'gem': 287, 'delightful': 288, 'spare': 289, 'wildlife': 290, 'traditionally': 291, 'structured': 292, 'adventurous': 293, 'indian': 294, 'filmmakers': 295, 'toward': 296, 'crossover': 297, 'nonethnic': 298, 'markets': 299, 'price': 300, 'admission': 301, 'gory': 302, 'mayhem': 303, 'idea': 304, 'stylish': 305, 'exercise': 306, 'hold': 307, 'grips': 308, 'hard': 309, 'fascinating': 310, 'byways': 311, 'evanescent': 312, 'seamless': 313, 'sumptuous': 314, 'stream': 315, 'slick': 316, 'manufactured': 317, 'claim': 318, 'street': 319, 'credibility': 320, 'take': 321, 'care': 322, 'nicely': 323, 'performed': 324, 'quintet': 325, 'actresses': 326, 'valuable': 327, 'messages': 328, 'thanks': 329, 'lau': 330, 'elevated': 331, 'hugh': 332, 'grant': 333, 'arrive': 334, 'early': 335, 'stay': 336, 'pan-american': 337, 'genuine': 338, 'insight': 339, 'urban': 340, 'shot': 341, 'artful': 342, 'watery': 343, 'tones': 344, 'blue': 345, 'green': 346, 'brown': 347, 'unorthodox': 348, 'organized': 349, 'includes': 350, 'strangest': 351, 'acted': 352, 'diane': 353, 'lane': 354, 'richard': 355, 'gere': 356, 'admittedly': 357, 'middling': 358, 'fighting': 359, 'skills': 360, 'steven': 361, 'seagal': 362, 'rich': 363, 'full': 364, 'warm': 365, 'water': 366, 'under': 367, 'red': 368, 'bridge': 369, 'celebration': 370, 'feminine': 371, 'energy': 372, 'power': 373, 'women': 374, 'heal': 375, 'real-life': 376, '19th-century': 377, 'metaphor': 378, 'home': 379, 'leave': 380, 'wanting': 381, 'mention': 382, 'leaving': 383, 'smile': 384, 'face': 385, 'retains': 386, 'ambiguities': 387, 'added': 388, 'resonance': 389, 'thing': 390, 'dot': 391, 'com': 392, 'résumé': 393, 'loaded': 394, 'credits': 395, 'girl': 396, 'bar': 397, '#': 398, '3': 399, 'add': 400, 'beyond': 401, 'dark': 402, 'visions': 403, 'already': 404, 'relayed': 405, 'predecessors': 406, 'yes': 407, 'snail-like': 408, 'pacing': 409, 'extremely': 410, 'unpleasant': 411, 'affair': 412, 'true': 413, 'incredibly': 414, 'hokey': 415, 'things': 416, 'we': 417, \"'ve\": 418, 'before': 419, 'predictable': 420, 'tides': 421, 'unassuming': 422, 'subordinate': 423, 'dim': 424, 'vicious': 425, 'absurd': 426, 'because': 427, 'acts': 428, 'goofy': 429, 'dentist': 430, 'waiting': 431, 'room': 432, 'amazingly': 433, 'lame': 434, 'confessions': 435, 'straightforward': 436, 'bio': 437, 'disappointingly': 438, 'thin': 439, 'slice': 440, 'lower-class': 441, 'london': 442, ';': 443, 'title': 444, 'bogus': 445, 'ugly': 446, 'shabby': 447, 'photography': 448, 'understand': 449, 'difference': 450, 'between': 451, 'plain': 452, 'frida': 453, 'different': 454, 'hollywood': 455, 'produced': 456, 'jerry': 457, 'bruckheimer': 458, 'directed': 459, 'joel': 460, 'schumacher': 461, 'reflects': 462, 'shallow': 463, 'styles': 464, 'overproduced': 465, 'inadequately': 466, 'motivated': 467, 'demographically': 468, 'targeted': 469, 'please': 470, 'carnage': 471, 'degraded': 472, 'handheld': 473, 'blair': 474, 'witch': 475, 'video-cam': 476, 'footage': 477, 'second': 478, 'fiddle': 479, 'folly': 480, 'superficiality': 481, 'auto-critique': 482, 'clumsiness': 483, 'damning': 484, 'censure': 485, 'tedious': 486, 'norwegian': 487, 'offering': 488, 'somehow': 489, 'snagged': 490, 'oscar': 491, 'nomination': 492, 'esther': 493, 'kahn': 494, 'unusual': 495, 'also': 496, 'irritating': 497, 'halfway': 498, 'beginning': 499, 'setup': 500, 'easy': 501, 'borders': 502, 'facile': 503, 'thoroughly': 504, 'smeary': 505, 'blurry': 506, 'distraction': 507, 'guns': 508, 'cheatfully': 509, 'filmed': 510, 'martial': 511, 'arts': 512, 'disintegrating': 513, 'bloodsucker': 514, 'computer': 515, 'jagged': 516, 'camera': 517, 'moves': 518, 'breezy': 519, 'distracted': 520, 'rhythms': 521, 'guys': 522, 'desperately': 523, 'quentin': 524, 'tarantino': 525, 'they': 526, 'grow': 527, 'up': 528, 'conceptions': 529, 'essentially': 530, 'collection': 531, 'bits': 532, 'bode': 533, 'rest': 534, 'usual': 535, 'used': 536, 'my': 537, 'hours': 538, 'john': 539, 'malkovich': 540, 'scant': 541, 'ludicrous': 542, 'ultra-cheesy': 543, 'handsome': 544, 'unfulfilling': 545, 'drama': 546, 'either': 547, 'her': 548, 'charmless': 549, 'everything': 550, 'girls': 551, 'ca': 552, 'swim': 553, 'passages': 554, 'observation': 555, 'secondhand': 556, 'familiar': 557, 'liked': 558, 'had': 559, 'gone': 560, 'further': 561, 'nicks': 562, 'steinberg': 563, 'match': 564, 'creations': 565, 'pure': 566, 'venality': 567, 'giving': 568, 'college': 569, 'try': 570, 'infectiously': 571, 'trying': 572, 'grab': 573, 'lump': 574, 'play-doh': 575, 'harder': 576, 'liman': 577, 'tries': 578, 'squeeze': 579, 'final': 580, 'effective': 581, 'stick': 582, 'promise': 583, 'filmmaking': 584, 'proves': 585, 'lovely': 586, 'trifle': 587, 'love': 588, 'classic': 589, 'casts': 590, 'actors': 591, 'magnificent': 592, 'landscape': 593, 'create': 594, 'feature': 595, 'wickedly': 596, 'trouble': 597, 'day': 598, 'preliminary': 599, 'notes': 600, 'science-fiction': 601, 'fragmentary': 602, 'narrative': 603, 'style': 604, 'piecing': 605, 'frustrating': 606, 'difficult': 607, 'altogether': 608, 'slight': 609, 'called': 610, 'masterpiece': 611, 'mid-to-low': 612, 'budget': 613, 'betrayed': 614, 'shoddy': 615, 'makeup': 616, 'collapse': 617, 'elvira': 618, 'fans': 619, 'hardly': 620, 'ask': 621, 'extraordinary': 622, 'faith': 623, 'ingenious': 624, 'compelling': 625, 'told': 626, 'scattered': 627, 'fashion': 628, 'pokes': 629, 'provokes': 630, 'expressionistic': 631, 'license': 632, 'caruso': 633, 'descends': 634, 'sub-tarantino': 635, 'sure': 636, 'salton': 637, 'sea': 638, 'works': 639, 'should': 640, 'keeping': 641, 'tight': 642, 'nasty': 643, 'unpretentious': 644, 'charming': 645, 'quirky': 646, 'original': 647, 'character': 648, 'dramas': 649, 'never': 650, 'reach': 651, 'satisfying': 652, 'conclusions': 653, 'vainly': 654, 'underscore': 655, 'importance': 656, 'tradition': 657, 'familial': 658, 'community': 659, 'woman': 660, 'great': 661, 'generosity': 662, 'diplomacy': 663, 'plodding': 664, 'provide': 665, 'keenest': 666, 'pleasures': 667, 'bring': 668, 'tissues': 669, 'interesting': 670, 'adaptation': 671, 'digital-effects-heavy': 672, 'supposed': 673, 'family-friendly': 674, 'realistic': 675, 'portrayal': 676, 'runs': 677, 'mere': 678, '84': 679, 'glance': 680, 'clumsy': 681, 'heavy-handed': 682, 'phoney-feeling': 683, 'sentiment': 684, 'grievous': 685, 'problem': 686, 'whether': 687, 'these': 688, 'ambitions': 689, 'laudable': 690, 'themselves': 691, 'inane': 692, 'harsh': 693, 'piece': 694, 'storytelling': 695, 'overbearing': 696, 'over-the-top': 697, 'yet': 698, 'grating': 699, 'showcase': 700, 'bon': 701, 'bons': 702, 'reveals': 703, 'important': 704, 'our': 705, 'talents': 706, 'service': 707, 'others': 708, 'color': 709, 'rather': 710, 'brings': 711, 'proper': 712, 'role': 713, 'bourne': 714, 'lacking': 715, 'surprise': 716, 'consistent': 717, 'emotional': 718, 'imax': 719, 'short': 720, 'eats': 721, 'meddles': 722, 'argues': 723, 'kibbitzes': 724, 'fights': 725, 'conclusive': 726, 'answers': 727, 'workable': 728, 'primer': 729, 'region': 730, 'terrific': 731, '10th-grade': 732, 'learning': 733, 'tool': 734, 'overlong': 735, 'bombastic': 736, 'tear': 737, 'eyes': 738, 'away': 739, 'images': 740, 'long': 741, 'read': 742, 'subtitles': 743, 'well-written': 744, 'well-acted': 745, 'build': 746, 'robots': 747, 'haul': 748, \"'em\": 749, 'theater': 750, 'show': 751, 'mystery': 752, 'science': 753, 'theatre': 754, '3000': 755, 'certainly': 756, 'going': 757, 'go': 758, 'down': 759, 'killer': 760, 'website': 761, 'other': 762, 'unpredictable': 763, 'those': 764, 'so-so': 765, 'films': 766, 'been': 767, 'do': 768, 'concert': 769, 'challenges': 770, 'poses': 771, 'forgive': 772, 'big': 773, 'excuse': 774, 'play': 775, 'lewd': 776, 'another': 777, 'lika': 778, 'da': 779, 'direction': 780, 'fluid': 781, 'no-nonsense': 782, 'authority': 783, 'performances': 784, 'harris': 785, 'phifer': 786, 'cam': 787, '`': 788, 'ron': 789, 'seal': 790, 'deal': 791, 'delivers': 792, 'promises': 793, 'wild': 794, 'ensues': 795, 'brash': 796, 'set': 797, 'conquer': 798, 'online': 799, 'world': 800, 'laptops': 801, 'cell': 802, 'phones': 803, 'business': 804, 'plans': 805, 'pleasant': 806, 'oozing': 807, 'uneven': 808, 'well-thought': 809, 'stunts': 810, 'spears': 811, \"'\": 812, 'music': 813, 'videos': 814, 'content': 815, 'except': 816, 'goes': 817, 'least': 818, '90': 819, 'worse': 820, 'see': 821, 'black': 822, 'ii': 823, 'achieves': 824, 'ultimate': 825, 'insignificance': 826, 'sci-fi': 827, 'spectacle': 828, 'whiffle-ball': 829, 'able': 830, 'hit': 831, '15-year': 832, \"'re\": 833, 'over': 834, '100': 835, 'starts': 836, 'off': 837, 'bad': 838, 'feel': 839, 'running': 840, 'screaming': 841, 'willing': 842, 'champion': 843, 'fallibility': 844, 'human': 845, 'respectable': 846, 'supremely': 847, 'unfunny': 848, 'unentertaining': 849, 'middle-age': 850, 'wide-awake': 851, 'hopeless': 852, 'dog': 853, 'lost': 854, 'translation': 855, 'remarkable': 856, 'procession': 857, 'sweeping': 858, 'pictures': 859, 'reinvigorated': 860, 'genre': 861, 'involved': 862, 'save': 863, 'dash': 864, 'shows': 865, 'slightest': 866, 'aptitude': 867, 'acting': 868, 'gender-bending': 869, 'generally': 870, 'quite': 871, 'funny': 872, 'lively': 873, 'engaging': 874, 'examination': 875, 'similar': 876, 'obsessions': 877, 'dominate': 878, 'welcome': 879, 'relief': 880, 'tasteful': 881, 'roll': 882, 'disney': 883, 'ransacks': 884, 'archives': 885, 'quick-buck': 886, 'sequel': 887, 'laugh': 888, 'maybe': 889, 'twice': 890, 'forgotten': 891, 'get': 892, 'back': 893, 'car': 894, 'parking': 895, 'lot': 896, 'there': 897, 'something': 898, 'artist': 899, '90-plus': 900, 'years': 901, 'taking': 902, 'effort': 903, 'share': 904, 'impressions': 905, 'loss': 906, 'art': 907, 'us': 908, 'absolutely': 909, 'ridiculous': 910, 'cobbled': 911, 'largely': 912, 'flat': 913, 'uncreative': 914, 'easily': 915, 'wait': 916, 'per': 917, 'view': 918, 'dollar': 919, 'irritates': 920, 'audacious': 921, 'hawaiian': 922, 'shirt': 923, 'addition': 924, 'sporting': 925, 'titles': 926, 'passable': 927, 'date': 928, 'movies': 929, 'suck': 930, 'hammily': 931, 'believe': 932, 'actually': 933, 'backseat': 934}\n",
            "935\n"
          ]
        }
      ],
      "source": [
        "## Construct a vocabulary\n",
        "from collections import Counter\n",
        "\n",
        "all_words = []\n",
        "for item in train_texts:\n",
        "  all_words += item.split()\n",
        "for item in dev_texts:\n",
        "  all_words += item.split()\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(all_words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {'<pad>':0, \"<unk>\":1}\n",
        "vocab_to_int.update({word: ii for ii, word in enumerate(vocab,2)})\n",
        "print(vocab_to_int)\n",
        "print(len(vocab_to_int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ0RsEBgqidq",
        "outputId": "600622e9-c4f5-4a3a-a492-329c1a33578d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Dataset Encode Examples\n",
            "[195  14   8 196   8 197  15   5 198 199  65   4  19  52  37   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0] 0\n",
            "[200 201   5 202 203 204 205  24   3  85  24  16  86  87   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0] 1\n",
            "[ 38 206  10  39 207  20 208 209   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0] 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def encode_sentence(sentence):\n",
        "    max_length = 50\n",
        "    input_ids = []\n",
        "    for item in sentence.split():\n",
        "      if item in vocab_to_int:\n",
        "        input_ids.append(vocab_to_int[item])\n",
        "      else:\n",
        "        input_ids.append(vocab_to_int['<unk>'])\n",
        "\n",
        "    padding_length = max_length - len(input_ids)\n",
        "    input_ids += [vocab_to_int['<pad>']] * padding_length\n",
        "    return np.array(input_ids)\n",
        "\n",
        "def encode_label(label):\n",
        "    return np.array(label)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Train Dataset Encode Examples\")\n",
        "for a,b in zip(train_texts[:3],train_labels[:3]):\n",
        "  print(encode_sentence(a),b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "-76p4FBbAqLp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleNN:\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.embedding_weights = np.random.rand(vocab_size, embedding_dim)  # Fixed embedding weights\n",
        "        limit = np.sqrt(6 / (embedding_dim + hidden_size))\n",
        "        self.W1 = np.random.uniform(-limit, limit, size=(embedding_dim, hidden_size))\n",
        "        #self.W1 = np.random.rand(embedding_dim, hidden_size)   # Input to hidden weights\n",
        "        limit = np.sqrt(6 / (hidden_size + output_size))\n",
        "        self.W2 = np.random.uniform(-limit, limit, size=(hidden_size, output_size))\n",
        "        #self.W2 = np.random.rand(hidden_size, output_size) # Hidden to output weights\n",
        "        self.b1 = np.zeros((1, hidden_size))  # Hidden layer biases\n",
        "        self.b2 = np.zeros((1, output_size))  # Output layer biases\n",
        "        self.X = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        return z * (1 - z)\n",
        "\n",
        "    def forward(self, X):\n",
        "      # Forward pass\n",
        "        ##############################################################################\n",
        "        embedding = np.mean(self.embedding_weights[X], axis=0, keepdims=True)  # 임베딩 벡터 평균\n",
        "        Z1 = np.dot(embedding, self.W1) + self.b1\n",
        "        A1 = self.sigmoid(Z1)\n",
        "        Z2 = np.dot(A1, self.W2) + self.b2\n",
        "        output = self.sigmoid(Z2)\n",
        "\n",
        "        self.A1 = A1  # Backward에서 사용하기 위해 저장\n",
        "        return output\n",
        "        ##############################################################################\n",
        "\n",
        "    def compute_loss(self, y, output):\n",
        "        # Compute binary cross-entropy loss\n",
        "        return -np.sum(y * np.log(output) + (1 - y) * np.log(1 - output))\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate=0.01):\n",
        "      ##############################################################################\n",
        "        dZ2 = output - y\n",
        "\n",
        "        dW2 = np.dot(self.A1.T, dZ2)\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "        dA1 = np.dot(dZ2, self.W2.T)\n",
        "        dZ1 = dA1 * self.sigmoid_derivative(self.A1)\n",
        "\n",
        "        embedding = self.embedding_weights[X].mean(axis=0).reshape(-1, 1)\n",
        "        dW1 = np.dot(embedding, dZ1)\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "      ##############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBBBnxduXd_u",
        "outputId": "65c9a6a2-084e-4ed0-f28d-1c7533545fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 epoch, train_loss = 0.7121, train_acc: 65.00%, eval_loss: 0.6979, eval_acc: 50.00%\n",
            "1 epoch, train_loss = 0.7110, train_acc: 63.00%, eval_loss: 0.6979, eval_acc: 50.00%\n",
            "2 epoch, train_loss = 0.7109, train_acc: 63.00%, eval_loss: 0.6978, eval_acc: 50.00%\n",
            "3 epoch, train_loss = 0.7109, train_acc: 62.00%, eval_loss: 0.6977, eval_acc: 50.00%\n",
            "4 epoch, train_loss = 0.7108, train_acc: 62.00%, eval_loss: 0.6976, eval_acc: 50.00%\n",
            "5 epoch, train_loss = 0.7107, train_acc: 62.00%, eval_loss: 0.6976, eval_acc: 50.00%\n",
            "6 epoch, train_loss = 0.7106, train_acc: 62.00%, eval_loss: 0.6975, eval_acc: 50.00%\n",
            "7 epoch, train_loss = 0.7106, train_acc: 62.00%, eval_loss: 0.6974, eval_acc: 50.00%\n",
            "8 epoch, train_loss = 0.7105, train_acc: 62.00%, eval_loss: 0.6973, eval_acc: 50.00%\n",
            "9 epoch, train_loss = 0.7104, train_acc: 62.00%, eval_loss: 0.6972, eval_acc: 50.00%\n",
            "10 epoch, train_loss = 0.7103, train_acc: 62.00%, eval_loss: 0.6972, eval_acc: 50.00%\n",
            "11 epoch, train_loss = 0.7103, train_acc: 62.00%, eval_loss: 0.6971, eval_acc: 50.00%\n",
            "12 epoch, train_loss = 0.7102, train_acc: 62.00%, eval_loss: 0.6970, eval_acc: 50.00%\n",
            "13 epoch, train_loss = 0.7101, train_acc: 62.00%, eval_loss: 0.6969, eval_acc: 50.00%\n",
            "14 epoch, train_loss = 0.7101, train_acc: 62.00%, eval_loss: 0.6969, eval_acc: 50.00%\n",
            "15 epoch, train_loss = 0.7100, train_acc: 62.00%, eval_loss: 0.6968, eval_acc: 50.00%\n",
            "16 epoch, train_loss = 0.7099, train_acc: 62.00%, eval_loss: 0.6967, eval_acc: 50.00%\n",
            "17 epoch, train_loss = 0.7099, train_acc: 62.00%, eval_loss: 0.6966, eval_acc: 50.00%\n",
            "18 epoch, train_loss = 0.7098, train_acc: 62.00%, eval_loss: 0.6966, eval_acc: 50.00%\n",
            "19 epoch, train_loss = 0.7097, train_acc: 62.00%, eval_loss: 0.6965, eval_acc: 50.00%\n",
            "20 epoch, train_loss = 0.7097, train_acc: 62.00%, eval_loss: 0.6964, eval_acc: 50.00%\n",
            "21 epoch, train_loss = 0.7096, train_acc: 62.00%, eval_loss: 0.6964, eval_acc: 50.00%\n",
            "22 epoch, train_loss = 0.7095, train_acc: 62.00%, eval_loss: 0.6963, eval_acc: 50.00%\n",
            "23 epoch, train_loss = 0.7094, train_acc: 62.00%, eval_loss: 0.6962, eval_acc: 50.00%\n",
            "24 epoch, train_loss = 0.7094, train_acc: 62.00%, eval_loss: 0.6961, eval_acc: 50.00%\n",
            "25 epoch, train_loss = 0.7093, train_acc: 62.00%, eval_loss: 0.6961, eval_acc: 50.00%\n",
            "26 epoch, train_loss = 0.7092, train_acc: 62.00%, eval_loss: 0.6960, eval_acc: 50.00%\n",
            "27 epoch, train_loss = 0.7092, train_acc: 62.00%, eval_loss: 0.6959, eval_acc: 50.00%\n",
            "28 epoch, train_loss = 0.7091, train_acc: 62.00%, eval_loss: 0.6959, eval_acc: 50.00%\n",
            "29 epoch, train_loss = 0.7091, train_acc: 62.00%, eval_loss: 0.6958, eval_acc: 50.00%\n",
            "30 epoch, train_loss = 0.7090, train_acc: 62.00%, eval_loss: 0.6958, eval_acc: 50.00%\n",
            "31 epoch, train_loss = 0.7089, train_acc: 62.00%, eval_loss: 0.6957, eval_acc: 50.00%\n",
            "32 epoch, train_loss = 0.7089, train_acc: 62.00%, eval_loss: 0.6956, eval_acc: 50.00%\n",
            "33 epoch, train_loss = 0.7088, train_acc: 62.00%, eval_loss: 0.6956, eval_acc: 50.00%\n",
            "34 epoch, train_loss = 0.7087, train_acc: 62.00%, eval_loss: 0.6955, eval_acc: 50.00%\n",
            "35 epoch, train_loss = 0.7087, train_acc: 62.00%, eval_loss: 0.6954, eval_acc: 50.00%\n",
            "36 epoch, train_loss = 0.7086, train_acc: 62.00%, eval_loss: 0.6954, eval_acc: 50.00%\n",
            "37 epoch, train_loss = 0.7085, train_acc: 62.00%, eval_loss: 0.6953, eval_acc: 50.00%\n",
            "38 epoch, train_loss = 0.7085, train_acc: 62.00%, eval_loss: 0.6953, eval_acc: 50.00%\n",
            "39 epoch, train_loss = 0.7084, train_acc: 62.00%, eval_loss: 0.6952, eval_acc: 50.00%\n",
            "40 epoch, train_loss = 0.7084, train_acc: 62.00%, eval_loss: 0.6951, eval_acc: 50.00%\n",
            "41 epoch, train_loss = 0.7083, train_acc: 62.00%, eval_loss: 0.6951, eval_acc: 50.00%\n",
            "42 epoch, train_loss = 0.7082, train_acc: 62.00%, eval_loss: 0.6950, eval_acc: 50.00%\n",
            "43 epoch, train_loss = 0.7082, train_acc: 62.00%, eval_loss: 0.6950, eval_acc: 50.00%\n",
            "44 epoch, train_loss = 0.7081, train_acc: 62.00%, eval_loss: 0.6949, eval_acc: 50.00%\n",
            "45 epoch, train_loss = 0.7081, train_acc: 62.00%, eval_loss: 0.6949, eval_acc: 50.00%\n",
            "46 epoch, train_loss = 0.7080, train_acc: 62.00%, eval_loss: 0.6948, eval_acc: 50.00%\n",
            "47 epoch, train_loss = 0.7079, train_acc: 62.00%, eval_loss: 0.6947, eval_acc: 50.00%\n",
            "48 epoch, train_loss = 0.7079, train_acc: 62.00%, eval_loss: 0.6947, eval_acc: 50.00%\n",
            "49 epoch, train_loss = 0.7078, train_acc: 62.00%, eval_loss: 0.6946, eval_acc: 50.00%\n",
            "50 epoch, train_loss = 0.7078, train_acc: 62.00%, eval_loss: 0.6946, eval_acc: 50.00%\n",
            "51 epoch, train_loss = 0.7077, train_acc: 62.00%, eval_loss: 0.6945, eval_acc: 50.00%\n",
            "52 epoch, train_loss = 0.7076, train_acc: 62.00%, eval_loss: 0.6945, eval_acc: 50.00%\n",
            "53 epoch, train_loss = 0.7076, train_acc: 62.00%, eval_loss: 0.6944, eval_acc: 50.00%\n",
            "54 epoch, train_loss = 0.7075, train_acc: 62.00%, eval_loss: 0.6944, eval_acc: 50.00%\n",
            "55 epoch, train_loss = 0.7075, train_acc: 62.00%, eval_loss: 0.6943, eval_acc: 50.00%\n",
            "56 epoch, train_loss = 0.7074, train_acc: 62.00%, eval_loss: 0.6943, eval_acc: 50.00%\n",
            "57 epoch, train_loss = 0.7074, train_acc: 62.00%, eval_loss: 0.6942, eval_acc: 50.00%\n",
            "58 epoch, train_loss = 0.7073, train_acc: 62.00%, eval_loss: 0.6942, eval_acc: 50.00%\n",
            "59 epoch, train_loss = 0.7072, train_acc: 62.00%, eval_loss: 0.6941, eval_acc: 50.00%\n",
            "60 epoch, train_loss = 0.7072, train_acc: 62.00%, eval_loss: 0.6941, eval_acc: 50.00%\n",
            "61 epoch, train_loss = 0.7071, train_acc: 62.00%, eval_loss: 0.6941, eval_acc: 50.00%\n",
            "62 epoch, train_loss = 0.7071, train_acc: 62.00%, eval_loss: 0.6940, eval_acc: 50.00%\n",
            "63 epoch, train_loss = 0.7070, train_acc: 62.00%, eval_loss: 0.6940, eval_acc: 50.00%\n",
            "64 epoch, train_loss = 0.7070, train_acc: 62.00%, eval_loss: 0.6939, eval_acc: 50.00%\n",
            "65 epoch, train_loss = 0.7069, train_acc: 62.00%, eval_loss: 0.6939, eval_acc: 50.00%\n",
            "66 epoch, train_loss = 0.7069, train_acc: 62.00%, eval_loss: 0.6938, eval_acc: 50.00%\n",
            "67 epoch, train_loss = 0.7068, train_acc: 62.00%, eval_loss: 0.6938, eval_acc: 50.00%\n",
            "68 epoch, train_loss = 0.7067, train_acc: 62.00%, eval_loss: 0.6937, eval_acc: 50.00%\n",
            "69 epoch, train_loss = 0.7067, train_acc: 62.00%, eval_loss: 0.6937, eval_acc: 50.00%\n",
            "70 epoch, train_loss = 0.7066, train_acc: 62.00%, eval_loss: 0.6937, eval_acc: 50.00%\n",
            "71 epoch, train_loss = 0.7066, train_acc: 62.00%, eval_loss: 0.6936, eval_acc: 50.00%\n",
            "72 epoch, train_loss = 0.7065, train_acc: 62.00%, eval_loss: 0.6936, eval_acc: 50.00%\n",
            "73 epoch, train_loss = 0.7065, train_acc: 62.00%, eval_loss: 0.6935, eval_acc: 50.00%\n",
            "74 epoch, train_loss = 0.7064, train_acc: 62.00%, eval_loss: 0.6935, eval_acc: 50.00%\n",
            "75 epoch, train_loss = 0.7064, train_acc: 62.00%, eval_loss: 0.6935, eval_acc: 50.00%\n",
            "76 epoch, train_loss = 0.7063, train_acc: 62.00%, eval_loss: 0.6934, eval_acc: 50.00%\n",
            "77 epoch, train_loss = 0.7063, train_acc: 62.00%, eval_loss: 0.6934, eval_acc: 50.00%\n",
            "78 epoch, train_loss = 0.7062, train_acc: 63.00%, eval_loss: 0.6934, eval_acc: 50.00%\n",
            "79 epoch, train_loss = 0.7062, train_acc: 63.00%, eval_loss: 0.6933, eval_acc: 50.00%\n",
            "80 epoch, train_loss = 0.7061, train_acc: 63.00%, eval_loss: 0.6933, eval_acc: 50.00%\n",
            "81 epoch, train_loss = 0.7061, train_acc: 63.00%, eval_loss: 0.6932, eval_acc: 50.00%\n",
            "82 epoch, train_loss = 0.7060, train_acc: 63.00%, eval_loss: 0.6932, eval_acc: 50.00%\n",
            "83 epoch, train_loss = 0.7060, train_acc: 63.00%, eval_loss: 0.6932, eval_acc: 50.00%\n",
            "84 epoch, train_loss = 0.7059, train_acc: 62.00%, eval_loss: 0.6931, eval_acc: 50.00%\n",
            "85 epoch, train_loss = 0.7059, train_acc: 62.00%, eval_loss: 0.6931, eval_acc: 50.00%\n",
            "86 epoch, train_loss = 0.7058, train_acc: 62.00%, eval_loss: 0.6931, eval_acc: 50.00%\n",
            "87 epoch, train_loss = 0.7058, train_acc: 62.00%, eval_loss: 0.6930, eval_acc: 50.00%\n",
            "88 epoch, train_loss = 0.7057, train_acc: 62.00%, eval_loss: 0.6930, eval_acc: 50.00%\n",
            "89 epoch, train_loss = 0.7057, train_acc: 62.00%, eval_loss: 0.6930, eval_acc: 50.00%\n",
            "90 epoch, train_loss = 0.7056, train_acc: 62.00%, eval_loss: 0.6930, eval_acc: 50.00%\n",
            "91 epoch, train_loss = 0.7056, train_acc: 62.00%, eval_loss: 0.6929, eval_acc: 50.00%\n",
            "92 epoch, train_loss = 0.7055, train_acc: 62.00%, eval_loss: 0.6929, eval_acc: 50.00%\n",
            "93 epoch, train_loss = 0.7055, train_acc: 62.00%, eval_loss: 0.6929, eval_acc: 50.00%\n",
            "94 epoch, train_loss = 0.7054, train_acc: 62.00%, eval_loss: 0.6928, eval_acc: 50.00%\n",
            "95 epoch, train_loss = 0.7054, train_acc: 61.00%, eval_loss: 0.6928, eval_acc: 50.00%\n",
            "96 epoch, train_loss = 0.7053, train_acc: 61.00%, eval_loss: 0.6928, eval_acc: 50.00%\n",
            "97 epoch, train_loss = 0.7053, train_acc: 61.00%, eval_loss: 0.6927, eval_acc: 50.00%\n",
            "98 epoch, train_loss = 0.7053, train_acc: 61.00%, eval_loss: 0.6927, eval_acc: 50.00%\n",
            "99 epoch, train_loss = 0.7052, train_acc: 61.00%, eval_loss: 0.6927, eval_acc: 50.00%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def train(X, y, learning_rate=0.01):\n",
        "    output = nn.forward(X)  # Forward pass\n",
        "    loss = nn.compute_loss(y, output)  # Compute loss\n",
        "    nn.backward(X, y, output, learning_rate)  # Backward pass\n",
        "    return loss\n",
        "\n",
        "def predict(x):\n",
        "    output = nn.forward(x)\n",
        "    return output, (output > 0.5).astype(int)  # Binary classification\n",
        "\n",
        "# Initialize the neural network\n",
        "vocab_size = len(vocab_to_int)  # Number of unique words in vocab\n",
        "embedding_dim = 100  # Embedding dimension\n",
        "hidden_size = 80  # Number of neurons in the hidden layer\n",
        "output_size = 1  # One output (binary classification)\n",
        "learning_rate = 0.01\n",
        "nn = SimpleNN(vocab_size, embedding_dim, hidden_size, output_size)\n",
        "\n",
        "true, pred = [], []\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    true, pred = [], []  # Reset true and predicted labels for each epoch\n",
        "    for x, y in zip(train_texts, train_labels):\n",
        "        x = encode_sentence(x)  # Encode sentence as word indices\n",
        "        train_loss += train(x, y, learning_rate)  # Train on the current sample\n",
        "        _, prediction = predict(x)  # obtain prediction\n",
        "        true.append(y)  # Append true label\n",
        "        pred.append(prediction[0][0])  # Append predicted label (extract scalar)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_acc = accuracy_score(true, pred) * 100.0\n",
        "    train_loss /= len(train_texts)  # Average training loss\n",
        "\n",
        "    # Evaluate on dev set\n",
        "    dev_true, dev_pred = [], []\n",
        "    dev_loss = 0.0\n",
        "    for x, y in zip(dev_texts, dev_labels):\n",
        "        x = encode_sentence(x)\n",
        "\n",
        "        output, prediction = predict(x)\n",
        "        dev_loss += nn.compute_loss(y,output)\n",
        "        dev_true.append(y)\n",
        "        dev_pred.append(prediction[0][0])\n",
        "    dev_loss /= len(dev_texts)\n",
        "    dev_acc = accuracy_score(dev_true, dev_pred) * 100.0\n",
        "    print(f'{epoch} epoch, train_loss = {train_loss:.4f}, train_acc: {train_acc:.2f}%, eval_loss: {dev_loss:.4f}, eval_acc: {dev_acc:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
